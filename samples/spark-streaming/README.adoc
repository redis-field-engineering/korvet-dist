= Spark Streaming Demo
:toc:
:toc-placement!:

Demonstrates Korvet's Kafka protocol compatibility with Apache Spark Structured Streaming.

toc::[]

== Overview

This demo shows how to use **Apache Spark Structured Streaming** to read real-time data from Korvet using the standard Kafka API. It demonstrates:

* ✅ **Kafka Protocol Compatibility** - Spark reads from Korvet as if it were Kafka
* ✅ **Structured Streaming** - Real-time stream processing with Spark
* ✅ **Multiple Queries** - Raw events, aggregations, and Parquet writes
* ✅ **Windowed Aggregations** - Time-based analytics on streaming data
* ✅ **Data Lake Integration** - Write streaming data to Parquet files

**Demo time:** 5 minutes

== Architecture

[source]
----
┌─────────────────┐
│ Data Generator  │ ──> Produces events
└────────┬────────┘
         │ Kafka Protocol
         ▼
┌─────────────────┐
│     Korvet      │ ──> Kafka-compatible API
└────────┬────────┘
         │ Stores in Redis Streams
         ▼
┌─────────────────┐
│ Redis Streams   │
└─────────────────┘
         │
         │ Kafka Protocol
         ▼
┌─────────────────┐
│ Spark Streaming │ ──> Reads & processes
└────────┬────────┘
         │
         ├──> Console (raw events)
         ├──> Console (aggregations)
         └──> Parquet files (data lake)
----

== Quick Start (5 Minutes)

=== Prerequisites

* Docker and Docker Compose
* 4GB+ RAM available
* Ports available: 6379, 8080, 8081, 9092

=== Start the Demo

[source,bash]
----
cd samples/spark-streaming

# Start all services (auto-builds Korvet if needed)
make up

# Run Spark Streaming application
make demo
----

You should see:

1. **Connection established** to Korvet
2. **Raw events** streaming to console
3. **Aggregated metrics** by event type (30-second windows)
4. **Parquet files** being written to `data/events/`

=== View the Results

Open the **Spark Master UI** to see running queries:

[source,bash]
----
make spark-ui
# Or open: http://localhost:8081
----
e d
View the **data generator** producing events:

[source,bash]
----
make logs-gen
----

Check **Redis Streams** to see stored data:

[source,bash]
----
make redis-cli
# Then run: XLEN acme:events:0
----

=== Stop the Demo

[source,bash]
----
make down
----

== What's Running

[cols="1,1,2"]
|===
|Service |Port |Description

|Redis
|6379
|Backend storage for Korvet

|Korvet
|9092, 8080
|Kafka-compatible streaming service

|Spark Master
|8081
|Spark cluster master node

|Spark Worker
|-
|Spark cluster worker node

|Data Generator
|-
|Produces sample e-commerce events
|===

== Sample Events

The data generator produces e-commerce events in JSON format:

[source,json]
----
{
  "event_id": "evt_1234567890123",
  "user_id": "user_42",
  "event_type": "purchase",
  "product_id": "laptop",
  "price": 1299.99,
  "timestamp": 1698765432000
}
----

**Event types:**

* `page_view` - User viewed a product page
* `add_to_cart` - User added item to cart
* `purchase` - User completed purchase
* `search` - User searched for products
* `click` - User clicked on element

== Spark Streaming Application

The demo runs three streaming queries simultaneously:

=== 1. Raw Events Query

Displays all events to the console in real-time.

[source,python]
----
events_df.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()
----

=== 2. Aggregated Events Query

Computes windowed aggregations (30-second windows) by event type:

* Event count per type
* Average price per type

[source,python]
----
aggregated_df = events_df \
    .withWatermark("kafka_timestamp", "10 seconds") \
    .groupBy(
        window(col("kafka_timestamp"), "30 seconds"),
        col("event_type")
    ) \
    .agg(
        count("*").alias("event_count"),
        avg("price").alias("avg_price")
    )
----

=== 3. Parquet Writer Query

Writes all events to Parquet files for data lake storage:

[source,python]
----
events_df.writeStream \
    .format("parquet") \
    .option("path", "/opt/spark-data/events") \
    .option("checkpointLocation", "/opt/spark-data/checkpoints/events") \
    .start()
----

== Makefile Commands

[source,bash]
----
# Setup
make up           # Start all services
make down         # Stop all services

# Demo
make demo         # Run Spark Streaming application
make generator    # View data generator status

# Spark
make spark-submit # Submit Spark job manually
make spark-shell  # Open PySpark shell
make spark-ui     # Open Spark Master UI

# Monitoring
make logs         # View all service logs
make logs-korvet  # View Korvet logs
make logs-spark   # View Spark logs
make logs-gen     # View data generator logs
make redis-cli    # Connect to Redis CLI

# Cleanup
make clean        # Stop services and remove all data
----



== Advanced Examples

=== Interactive PySpark Shell

Open an interactive PySpark shell connected to Korvet:

[source,bash]
----
make spark-shell
----

Then try these commands:

[source,python]
----
# Read from Korvet
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "korvet:9092") \
    .option("subscribe", "events") \
    .option("startingOffsets", "earliest") \
    .load()

# Parse JSON and display
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StructType, StructField, StringType, DoubleType

schema = StructType([
    StructField("event_id", StringType()),
    StructField("user_id", StringType()),
    StructField("event_type", StringType()),
    StructField("price", DoubleType())
])

events = df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Start streaming query
query = events.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
----

=== Custom Spark Application

Create your own Spark application:

[source,python]
----
# my_app.py
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyKorvetApp") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0") \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "korvet:9092") \
    .option("subscribe", "events") \
    .load()

# Your processing logic here...
----

Submit it:

[source,bash]
----
# Add to docker-compose.yml volumes:
# - ./my_app.py:/opt/spark-apps/my_app.py

docker compose exec spark-master \
    spark-submit \
    --master spark://spark-master:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
    /opt/spark-apps/my_app.py
----

=== Batch Processing

Read Kafka data in batch mode:

[source,python]
----
df = spark.read \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "korvet:9092") \
    .option("subscribe", "events") \
    .option("startingOffsets", "earliest") \
    .option("endingOffsets", "latest") \
    .load()

df.show()
----

=== Write to Korvet

Spark can also write to Korvet using the Kafka API:

[source,python]
----
# Create a DataFrame
data = [("key1", "value1"), ("key2", "value2")]
df = spark.createDataFrame(data, ["key", "value"])

# Write to Korvet
df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
    .write \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "korvet:9092") \
    .option("topic", "output-topic") \
    .save()
----

== Troubleshooting

=== Services Not Starting

**Problem:** Docker Compose fails to start services.

**Solution:**

[source,bash]
----
# Check if ports are already in use
lsof -i :6379  # Redis
lsof -i :8080  # Korvet HTTP
lsof -i :8081  # Spark UI
lsof -i :9092  # Korvet Kafka

# Stop conflicting services or change ports in docker-compose.yml
----

=== Korvet Image Not Found

**Problem:** `Error: pull access denied for korvet`

**Solution:**

The sample uses `redisfield/korvet` from Docker Hub. Docker will automatically pull it on first run. If you see this error, try:

[source,bash]
----
docker pull redisfield/korvet
----

=== No Data in Spark Streaming

**Problem:** Spark Streaming shows no data.

**Solution:**

1. Check if data generator is running:
+
[source,bash]
----
make logs-gen
----

2. Verify data in Redis:
+
[source,bash]
----
make redis-cli
# Then: XLEN acme:events:0
----

3. Check Korvet logs:
+
[source,bash]
----
make logs-korvet
----

4. Ensure `startingOffsets` is set to `"earliest"` in your Spark code


=== Spark Job Fails

**Problem:** Spark job fails with connection errors.

**Solution:**

1. Ensure all services are healthy:
+
[source,bash]
----
docker compose ps
----

2. Wait longer for services to start (30+ seconds)

3. Check Spark logs:
+
[source,bash]
----
make logs-spark
----

=== Out of Memory

**Problem:** Spark worker runs out of memory.

**Solution:**

Edit `docker-compose.yml` to increase worker memory:

[source,yaml]
----
spark-worker:
  environment:
    - SPARK_WORKER_MEMORY=2G  # Increase from 1G
----

Then restart:

[source,bash]
----
make down
make up
----

== Performance Tuning

=== Increase Parallelism

Adjust Spark parallelism for better throughput:

[source,python]
----
spark = SparkSession.builder \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.default.parallelism", "100") \
    .getOrCreate()
----

=== Kafka Consumer Options

Tune Kafka consumer settings:

[source,python]
----
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "korvet:9092") \
    .option("subscribe", "events") \
    .option("maxOffsetsPerTrigger", "10000") \
    .option("minPartitions", "4") \
    .load()
----

=== Checkpoint Location

Always specify a checkpoint location for production:

[source,python]
----
query = df.writeStream \
    .option("checkpointLocation", "/opt/spark-data/checkpoints/my-query") \
    .start()
----

== Why This Works

Korvet implements the **Kafka wire protocol**, which means:

* ✅ **No code changes** - Spark uses standard Kafka connector
* ✅ **Full compatibility** - All Kafka features work (offsets, consumer groups, etc.)
* ✅ **Transparent** - Spark doesn't know it's talking to Korvet, not Kafka
* ✅ **Cost-effective** - Data stored in Redis Streams, not expensive Kafka brokers

Behind the scenes:

1. Spark sends Kafka protocol requests to Korvet (port 9092)
2. Korvet translates to Redis Streams operations (XREAD, XRANGE)
3. Data flows back through Kafka protocol responses
4. Spark processes data using standard Structured Streaming APIs

== Next Steps

* **Modify the Spark application** - Edit `streaming_reader.py`
* **Change event schema** - Update `generate-data.sh`
* **Add more queries** - Implement custom analytics
* **Integrate with data lake** - Write to S3, Delta Lake, or Iceberg
* **Try other Spark features** - Machine learning, graph processing, etc.

== Learn More

* link:../../README.adoc[Korvet Project Overview]
* link:../README.adoc[All Samples]
* https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html[Spark-Kafka Integration Guide]
* https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html[Structured Streaming Guide]
