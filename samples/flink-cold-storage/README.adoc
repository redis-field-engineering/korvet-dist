= Flink Cold Storage Writer

Kubernetes manifests for deploying the Korvet Flink cold storage writer.

== Overview

The cold storage writer archives messages from Redis Streams to Delta Lake for long-term retention.

== Files

* `configmap.yaml` - Configuration settings (Redis URI, S3 path, key pattern)
* `secret.yaml` - AWS credentials (optional with IAM roles)
* `serviceaccount.yaml` - Kubernetes service account
* `flink-application.yaml` - FlinkDeployment CRD (requires Flink Kubernetes Operator)
* `job.yaml` - Plain Kubernetes Job (alternative to Flink Operator)
* `kustomization.yaml` - Kustomize configuration

== Quick Start

=== With Flink Kubernetes Operator

[source,bash]
----
# Install Flink Operator
helm repo add flink-operator https://downloads.apache.org/flink/flink-kubernetes-operator-1.10.0/
helm install flink-operator flink-operator/flink-kubernetes-operator

# Edit configmap.yaml and secret.yaml with your settings
# Then deploy:
kubectl apply -k .
----

=== Without Flink Operator

[source,bash]
----
# Edit configmap.yaml and secret.yaml with your settings
kubectl apply -f configmap.yaml
kubectl apply -f secret.yaml
kubectl apply -f serviceaccount.yaml
kubectl apply -f job.yaml
----

== Configuration

Edit `configmap.yaml` to set:

* `REDIS_URI` - Redis connection string
* `STORAGE_PATH` - Delta Lake path (e.g., `s3a://bucket/delta`)
* `KEY_PATTERN` - Redis stream key pattern to archive
* `S3_REGION` - AWS region

For static AWS credentials, edit `secret.yaml`. On EKS with IRSA, you can omit credentials and use IAM roles instead.

== Documentation

See https://redis.io/docs/latest/integrate/korvet/storage/cold-storage/[Cold Storage Documentation] for details.

